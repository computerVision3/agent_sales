# from fastapi import APIRouter
# from pydantic import BaseModel
# from agents.nodes import chat_session, add_user_message
# from agents.graph import graph
# from agents.agent_state import AgentState

# import logging

# logger = logging.getLogger(__name__)


# router = APIRouter(tags=["Start Chat"])

# class ChatRequest(BaseModel):
#     session_id: str
#     user_input: str

# @router.post("/chat",
#              summary="Start working with Agent",
#              description="""
#     This endpoint allows a user to interact with an AI agent. 
#     Given a session ID and a user input message:
    
#     Request Body:
#     - session_id (str): Unique identifier for the chat session.
#     - user_input (str): The message input from the user.
    
#     Response:
#     - status (str): Indicates success of the operation ("ok").
#     - conversation (List[str]): List of messages generated by the agent during this interaction.
#     """             
#              )

# async def chat_endpoint(request: ChatRequest):

#     chat_history = await chat_session(request.session_id)


#     state: AgentState = {
#         "messages": [],
#         "tool_outputs": {},
#         "chat_history": chat_history
#     }

#     await add_user_message(request.user_input, state, chat_history)


#     # 4. Run LangGraph
#     conversation = []
#     async for new_state in graph.astream(state, stream_mode="values"):
#         last_msg = new_state.get("messages", [])[-1] if new_state.get("messages") else None
#         if last_msg:
#             conversation.append(last_msg.content)

#     return {"conversation": conversation}


##################################################################################
######################### Stream Chat Endpoint ###################################
##################################################################################

from fastapi import APIRouter
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import json
from agents.nodes import chat_session, add_user_message
from agents.graph import graph
from agents.agent_state import AgentState
from langchain_core.messages import ToolMessage

import logging

logger = logging.getLogger(__name__)
router = APIRouter(tags=["Start Chat"])

class ChatRequest(BaseModel):
    session_id: str
    user_input: str

@router.post("/chat")
async def chat_endpoint(request: ChatRequest):
    async def generate_tokens():
        try:
            # -----------------------------
            # Get chat history and in-memory messages
            # -----------------------------
            chat_history, state_messages = await chat_session(request.session_id)
            
            # Initialize state with in-memory messages and DB object
            state: AgentState = {
                "messages": state_messages,  # includes system message only in memory
                "tool_outputs": {},
                "chat_history": chat_history
            }
            
            # -----------------------------
            # Add user message
            # -----------------------------
            await add_user_message(request.user_input, state, chat_history)
            
            # -----------------------------
            # Prepare for streaming
            # -----------------------------
            buffer = ""
            in_tool_call = False
            
            # Stream tokens from the graph
            async for token, metadata in graph.astream(state, stream_mode="messages"):
                # Get token content
                token_content = getattr(token, 'content', token)
                if not token_content:
                    continue
                
                # -----------------------------
                # Handle tool call start
                # -----------------------------
                if hasattr(token, 'tool_calls') and token.tool_calls:
                    in_tool_call = True
                    yield f"data: {json.dumps({'type': 'tool_call', 'tool': token.tool_calls[0]['name']})}\n\n"
                    continue
                
                # -----------------------------
                # Handle tool result
                # -----------------------------
                if isinstance(token, ToolMessage):
                    in_tool_call = False
                    yield f"data: {json.dumps({'type': 'tool_result', 'content': token_content})}\n\n"
                    continue
                
                # Skip streaming reasoning tokens during a tool call
                if in_tool_call:
                    continue
                
                # -----------------------------
                # Accumulate buffer
                # -----------------------------
                buffer += token_content
                if len(buffer) >= 10 or buffer.endswith(('.', '!', '?', '\n\n')):
                    yield f"data: {json.dumps({'type': 'content', 'content': buffer})}\n\n"
                    buffer = ""
            
            # Send any remaining buffer
            if buffer:
                yield f"data: {json.dumps({'type': 'content', 'content': buffer})}\n\n"
            
            # Signal end of stream
            yield "data: {\"type\": \"end\"}\n\n"
        
        except Exception as e:
            logger.error(f"Error in streaming: {str(e)}")
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"
    
    return StreamingResponse(
        generate_tokens(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
        }
    )
